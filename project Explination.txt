With AMEX,i work for business travel,where customers traveling from different location based on their business.
It colud be small,medium,corporate(large-scale) business clients.AMEX provides different credit cards to customers.
who ever using our business cards for different travel purpose within the business area ,we will grt those cistomer information.That is the business i handle.
1.Here i will get information from different sources like JSON File, XML File and Oracle(existing database)
2.For JSON we have an internal API,that will grap the data from portal and it will make it as json File.
3.Byusing SQOOP we will import data into HDFS.
4.Once data ingested into HDFS,we will do some validations on that data.Like Schema validations and Threshold limits

Threshold limit means->everyday we will get data  from oracle daily like 100,000 records if its less than certain level  the job will fail.we need to verify with the source system whether the count is expected or any other changes on the data.(**I need some explination on this point)
5.After validations done,we will store it into hive.
6.we don't do any update,delete in hive.
7.some tables need to do transformations,joins depends on business requirement.So we r using Pyspark to do transformations.
8.Spark will read data from HDFS location and directly write into rawzone.
9.depending on business requirement we will ingest some data into our data lake(we will provide to business user)
10.and some data to Marketing Team->they need some specific data from specific tables.They have some requirement on daily update on some columns.


